{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Natural language processing using *Spacy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Spacy* (https://spacy.io/usage/linguistic-features) is another Python module for *natural language processing (nlp)*.\n",
    "\n",
    "*Spacy* does much of the same analyses that *TextBlob* does, but also includes *named entity recognition* (discussed below). While the choice between *TextBlob* and *Spacy* is a matter of preference for most analyses, a general rule of thumb for us will be to use *TextBlob* for sentiment analysis and *Spacy* for named entity recognition.\n",
    "\n",
    "In order to use *Spacy*, a language model (https://spacy.io/models/en) must be loaded. Then natural language processing is carried out, as in the following example:\n",
    "\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('text to analyze')\n",
    "```\n",
    "\n",
    "By convention the loaded language model is stored in an object named *nlp*, and the processed text is stored in an object named *doc*, which is a sequence of *token* objects. Each token contains various information such as its *lemma* and *part-of-speech*, which can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# import spacy and load language model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# description of Eastern from Wikipedia\n",
    "eastern = \"\"\"\n",
    "Eastern Connecticut State University is a public liberal arts university in \n",
    "Willimantic, Connecticut. Founded in 1889, it is the second-oldest campus in \n",
    "the Connecticut State University System and third-oldest public university in \n",
    "the state. Eastern is located on Windham Street in Willimantic, Connecticut, \n",
    "on 182 acres (0.74 km2) 30 minutes from Hartford, lying midway between New York \n",
    "City and Boston.\n",
    "\"\"\"\n",
    "\n",
    "# process the text\n",
    "doc = nlp(eastern)\n",
    "\n",
    "# doc is a sequence of token objects, but displays as text when printed or evaluated\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Each *token* in the *doc* object contains the following properties (for a full list see https://spacy.io/api/token#attributes):\n",
    "\n",
    "- *token.string*: the word text\n",
    "- *token.lemma_*: the lemma or base form of the word\n",
    "- *token.tag_*: the detailed part of speech tag (e.g., NN is a singular noun and NNS is a plural noun) \n",
    "- *token.pos_*: the simple part of speech tag (e.g., all singular and plural nouns are both labeled NOUN)\n",
    "\n",
    "\n",
    "The code below uses list comprehension to first create a tuple containing the string, lemma, tag, and pos. Then we display the results using a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Use list comprehension to create a tuple containing the string, lemma, tag, and pos for \n",
    "# the first 20 tokens. \n",
    "tokens = [(token.string, token.lemma_, token.tag_, token.pos_) for token in doc[:20]]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# create a pandas data frame to display the results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tokens, columns = ['token', 'lemma', 'tag', 'pos'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Named entity recognition\n",
    "\n",
    "A named entity consists of a noun (or phrase) that corresponds a predefined category such as a \"person\", a \"date\", or an \"organization\". Named entities can be accessed through \n",
    "```\n",
    "doc.ents\n",
    "```\n",
    "which returns a tuple of tokens. Each *token* will have the following properties:\n",
    "\n",
    "- *ent.text*: the text of the named entity\n",
    "- *ent.label_*: the label for the named entity\n",
    "\n",
    "A list of the named entities can be found here: https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# create a list of tuples, containing the entity and its label, and display in a data frame\n",
    "entities = [ (ent.text, ent.label_) for ent in doc.ents]\n",
    "pd.DataFrame(entities, columns = ['Text','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing named entities\n",
    "\n",
    "The *displaCy* visualizer can be used to display the text with named entities highlighted, as in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Noun phrase extraction\n",
    "In *Spacy*, noun phrases are called *noun chunks* and are available in *doc.noun_chunks*.\n",
    "Each *chunk* contains the following:\n",
    "- *chunk.text* - the text of the noun chunk\n",
    "- *chunk.root* - the root word of the noun chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "chunks = [ (chunk.text, chunk.root) for chunk in doc.noun_chunks]\n",
    "df = pd.DataFrame(chunks, columns = ['Noun Chunk', 'Root'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
